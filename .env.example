# Database
DATABASE_URL=postgresql+asyncpg://nexus:nexus@postgres:5432/nexus

# Vector Database
QDRANT_URL=http://qdrant:6333
QDRANT_API_KEY=

# Cache
REDIS_URL=redis://redis:6379

# AI Providers (at least one recommended, Ollama works without API key)
OPENAI_API_KEY=
GEMINI_API_KEY=
OLLAMA_URL=http://ollama:11434

# Ollama Model Configuration
# Embedding model options: nomic-embed-text (768d), mxbai-embed-large (1024d), all-minilm (384d)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# LLM model options: llama3.1:8b, llama3.2, mistral, qwen2.5, gemma2
OLLAMA_LLM_MODEL=qwen2.5vl:3b
# Must match the embedding model's dimension (nomic-embed-text=768, mxbai-embed-large=1024)
OLLAMA_EMBEDDING_DIMENSION=768
# Context window size for LLM calls (llama3.1:8b=128k, llama3.2=128k, mistral=32k)
# Conservative default to ensure compatibility and avoid truncation
OLLAMA_CONTEXT_WINDOW=4096

# Gemini Model Configuration
GEMINI_EMBEDDING_MODEL=models/text-embedding-004
GEMINI_LLM_MODEL=gemini-2.5-flash-exp
GEMINI_EMBEDDING_DIMENSION=768
GEMINI_CONTEXT_WINDOW=32768

# OpenAI Model Configuration
# Embedding model options: text-embedding-3-small (1536d), text-embedding-3-large (3072d)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
# LLM model options: gpt-4o-mini, gpt-4o
OPENAI_LLM_MODEL=gpt-4o-mini
# Must match embedding model (small=1536, large=3072)
OPENAI_EMBEDDING_DIMENSION=1536
OPENAI_CONTEXT_WINDOW=8192

# Cross-Encoder Re-ranking
# Enable cross-encoder re-ranking for improved search relevance (default: true)
ENABLE_RERANKING=true
# HuggingFace model for re-ranking (bge-reranker-base, ms-marco-MiniLM-L-6-v2)
RERANKER_MODEL=BAAI/bge-reranker-base
# Maximum token length for cross-encoder model (bge-reranker-base=512, ms-marco=512)
RERANKER_MAX_LENGTH=512
# Number of top candidates to re-rank (higher = better quality, slower)
RERANKER_TOP_K=20

# Qdrant Storage
# Maximum character length for parent_content in Qdrant payloads (prevents HTTP timeouts during batch uploads)
QDRANT_MAX_PAYLOAD_SIZE=5000

# Metadata Extraction
# Reserved tokens for prompts and response overhead in metadata extraction
METADATA_PROMPT_OVERHEAD=800
# Maximum tokens for quick chunk metadata extraction (smaller = faster processing)
METADATA_CHUNK_MAX_TOKENS=500

# Source Integrations
GITHUB_TOKEN=
GMAIL_CLIENT_ID=
GMAIL_CLIENT_SECRET=
GMAIL_REDIRECT_URI=http://localhost:8000/auth/gmail/callback

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
LOG_LEVEL=INFO

# Security
SECRET_KEY=change-me-in-production-use-openssl-rand-hex-32
API_KEY_SALT=change-me-too-use-openssl-rand-hex-32

# Performance
MAX_WORKERS=4
BATCH_SIZE=100
CHUNK_SIZE=8192
CHUNK_OVERLAP=500

# Costs (for analytics)
OPENAI_EMBED_COST_PER_1K=0.0001
GEMINI_EMBED_COST_PER_1K=0.00001
OLLAMA_EMBED_COST_PER_1K=0.0

# Adaptive RAG Configuration
# Enable LLM-based query planning for adaptive retrieval strategies (currently experimental, set to false by default)
ENABLE_ADAPTIVE_RAG=false
# Enable query decomposition for complex/multi-faceted questions
ENABLE_QUERY_DECOMPOSITION=true
# Enable iterative retrieval with LLM self-assessment
ENABLE_ITERATIVE_RAG=true
# Maximum number of sub-queries when decomposing complex queries (2-10)
MAX_SUB_QUERIES=5
# Maximum iterations for iterative RAG (1-5)
MAX_RAG_ITERATIONS=3
