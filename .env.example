# Database
DATABASE_URL=postgresql+asyncpg://nexus:nexus@postgres:5432/nexus

# Vector Database
QDRANT_URL=http://qdrant:6333
QDRANT_API_KEY=

# Cache
REDIS_URL=redis://redis:6379

# AI Providers (at least one recommended, Ollama works without API key)
OPENAI_API_KEY=
GEMINI_API_KEY=
OLLAMA_URL=http://ollama:11434

# Ollama Model Configuration
# Embedding model options: nomic-embed-text (768d), mxbai-embed-large (1024d), all-minilm (384d)
OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# LLM model options: llama3.1:8b, llama3.2, mistral, qwen2.5, gemma2
OLLAMA_LLM_MODEL=qwen2.5vl:3b
# Must match the embedding model's dimension (nomic-embed-text=768, mxbai-embed-large=1024)
OLLAMA_EMBEDDING_DIMENSION=768
# Context window size for LLM calls (llama3.1:8b=128k, llama3.2=128k, mistral=32k)
# Conservative default to ensure compatibility and avoid truncation
OLLAMA_CONTEXT_WINDOW=4096

# Cross-Encoder Re-ranking
# Enable cross-encoder re-ranking for improved search relevance (default: true)
ENABLE_RERANKING=true
# HuggingFace model for re-ranking (bge-reranker-base, ms-marco-MiniLM-L-6-v2)
RERANKER_MODEL=BAAI/bge-reranker-base
# Maximum token length for cross-encoder model (bge-reranker-base=512, ms-marco=512)
RERANKER_MAX_LENGTH=512
# Number of top candidates to re-rank (higher = better quality, slower)
RERANKER_TOP_K=20

# Qdrant Storage
# Maximum character length for parent_content in Qdrant payloads (prevents HTTP timeouts during batch uploads)
QDRANT_MAX_PAYLOAD_SIZE=5000

# Source Integrations
GITHUB_TOKEN=
GMAIL_CLIENT_ID=
GMAIL_CLIENT_SECRET=
GMAIL_REDIRECT_URI=http://localhost:8000/auth/gmail/callback

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_WORKERS=4
LOG_LEVEL=INFO

# Security
SECRET_KEY=change-me-in-production-use-openssl-rand-hex-32
API_KEY_SALT=change-me-too-use-openssl-rand-hex-32

# Performance
MAX_WORKERS=4
BATCH_SIZE=100
CHUNK_SIZE=8192
CHUNK_OVERLAP=500

# Costs (for analytics)
OPENAI_EMBED_COST_PER_1K=0.0001
GEMINI_EMBED_COST_PER_1K=0.00001
OLLAMA_EMBED_COST_PER_1K=0.0
